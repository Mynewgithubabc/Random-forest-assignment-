Analysing the functions: 

Explain how each function decides the value of y based on x1, x2 and x3. Do you think these data-generating mechanisms are consistent with the structure of a decision tree? Why?


```{r}
data.generator.1 <- function(N) { #we are making a funciton called data generator 
  x1 <- rnorm(N,0,1) #predictor 1 #we are making a variable called x1, which is used to decide y: we are drawing the value of x from a normal distribution? 
  x2 <- rnorm(N,0,1) #predictor 2: same as above 
  x3 <- rnorm(N,0,1) #predictor 3 
  
  class <- rep(0, each=N) #outcome (to be filled below)  making a new object called class. We decide what class each i is in based on the value of x1, x2 and x3 
  #assigns a class to each data point
  for ( i in 1:N ) { #for every i / object in 1/N that it samples 
    if ( x1[i] >0 ) { #if the variable of x1 of i is greater than 0
      if (x2[i] >0 ) { #and if the variable of x2 of i is greater than 0
        class[i] = 1 #put it in class 1 
      }
      else
        class[i] = 0 #if this condition is not met, put it in class 0 
    }
    else #but 
      if ( x3[i] > 0 ) { #if the value of x3 of i is greater than 0 
        class[i] = 1 #put it in class 1 
      }
    else
      class[i] = 0 #if not, put it in class 0 
  }
  class <- factor(class) #making an object called class where being in class 0 or 1 is a factor variable, this is y 
  data <- data.frame(x1, x2, x3, class) #making a data frame which is made up of 4 variables, values of x1, x2 and x3, and what class you are in 
  data
  
}
```

data.generator.1: 

The function data.generator.1 is drawing objects from a dataset of N number of objects. For each object i in N, it then makes three continuous variables caled x1, x2 and x3. The values of x1 x2 and x3 assigned to each value of i are taken from a random sample of a normal distribution. The function then makes a second variable for each object i in N called class, which will be the y variable. It initially sets the value of class for each object i to 0. 

The function then has a for loop which is used to decides whether the value of class of the data point i in N is 0 or 1, depending on the values of x1, x2 and x3. The for loop states that if the value of x1 >0, the value of x2 should be assessed. If the value of X2 > 0, class is set to 1. The 'else' part of the for loop states that if x1 is less or equal to 0, the value of x3 should be assessed. If x3 > 0, class is set to 1. If x3 is less or equal to 0, class remains at 0. 

Do you think these data-generating mechanisms are consistent with the structure of a decision tree? Why?

Decision trees define thresholds around observed values of variables which results in divisions of data. They use recursive binary splitting processes, meaning a series of decisions are taken around whether the values of the variables fall above or below a certain value until the end nodes of the decision are pure. This data generating mechanism is consistent with the structure of a decision tree. The function uses the value of three variables, x1, x2, and x3 and assigns the value of the categorical variable y based on the outcome of whether the values of x1, x2 and x3 fall above or below a fixed value, 0. The end result of the function is that the data is divided into fixed groups where the nodes are pure, as the value of y (class) has either been assigned to 0 or 1.

Consider whether it is apporpirate to talk about binary splits, splits can be multiway? Greater or equal? 

data.generator.2 

```{r}

data.generator.2 <- function( N ) { #making a function defining characteristics about N 
  x1 <- rnorm(N,0,1) #predictor 1: made a variable called x1, we assign values of x1 by drawing randomly from the normal distribution 
  x2 <- rnorm(N,0,1) #predictor 2
  x3 <- rnorm(N,0,1) #predictor 3 
  class <- rep(0, each=N) #outcome (to be fulfilled below): we have made the variable class, right now every data point i in N is class 0 but the true outcome of class is decided by the outcome of the foor loop below: 
  #assigns a class to each data point 
  for ( i in 1:N ) { #for every data point i in N 
    if ( x1[i] >0 ) { #if x1 value is greater than 0
      if ( x3[i] > 0 ) { #then assess the value of x3. If x3 > 0, it is class 1 
        class[i] = 1
      }
      else
        class[i] = 0 #if this condition is not fulfilled, so if value of x1 is not greater than 0 or if the value of x3 is not greater than 0, put it in class 0.
    }
    else
      class[i] = 0 #if none of these conditions are met, put it in class 0 
  }
  class <- factor(class)
  data <- data.frame(x1, x2, x3, class)
  data
}

```

Explain how each function decides the value of y based on x1, x2 and x3.

In the same way as the function data.generator.1, data.generator.2 draws objects from a dataset of N number of objects. For each object i in N, it then makes three continuous variables caled x1, x2 and x3. The values of x1 x2 and x3 assigned to each value of i are taken from a random sample of a normal distribution. The function then makes a second variable for each object i in N called class, which will be the y variable. It initially sets the value of class for each object i to 0. 

The function then has a for loop which is used to decides whether the value of class of the data point i in N is 0 or 1, depending on the values of x1 and x3. The for loop states that for each data point in N, if the value of x1 is greater than 0, the value of x3 should be assessed. If the value of x3 is greater than 0, class should be assigned as 1. If the value of x1 or x3 is less than 0, class should be assigned as 0. If a condition other than these outcomes is met (such as any value of x2), class should still be assigned as 0. 

Do you think these data-generating mechanisms are consistent with the structure of a decision tree? Why? 

Yes this data generating mechanism is consistent with the structure of a decision tree. Although it only uses information about two out of three of the variables, x1 and x3, the tree still uses a recursive binary splitting process. This means that a series of decisions is taken based on whether the values of x1 and x3 fall above or below a certain value, 0, until the data are divided into fixed groups where the end nodes of the decision tree are pure. This means that the outcome of the decision tree is binary; any data point i in N is either in class 1 or class 0. Using some but not all of the possible variables to decide the final outcome in a decision tree is common practice in Random Forest. Random forest uses many decision trees on the same set of data which use different variables in order to avoid overfitting. 


Question 2: 

```{r}
#Loading the required packages: 
#install.packages("cowplot")
#install.packages("randomForest")
#install.packages("pROC")
#install.packages("reprtree")
#(later on make an if loop which installs the packages if they are not installed)
library(tidyverse)
library(cowplot)
library(randomForest)
library(pROC)
library(reprtree)

#The package __reprtree__ can be installed with the following instructions.

#install.packages("devtools")
#devtools::install_github('araastat/reprtree')

```

```{r}
# Generate 200 data points
dataset_question_2 <- data.generator.1(200)

# View the first few rows of the generated data
head(dataset_question_2)


```

Running a random forest model to predict y given the values of x1, x2 and x3. We are feeding the data and the outcomes to our model so that our model learns how to predict the outcomes from new data based on this. 

```{r}

##make sure that the variable we are predicting is a factor
##which tells the software we are doing a classification exercise
#we need to make sure the native speaker column is a factor variable! Converting it to a factor variable which is important otherwise random forest wont work 
dataset_question_2$class<- factor(dataset_question_2$class)

rf <- randomForest(class ~ ., data = dataset_question_2)
rf

```
Exploring random forest parameters: 



```{r}

##force a large number of trees
rf <- randomForest(class ~ ., data = dataset_question_2, ntree=1000)

##this is how the OOB error rates are extracted from the solution within "rf"
head(rf$err.rate)
```

The output __rf$err.rate__ provides an estimate of the error for the entire dataset (OOB, column 1), and the two available values of the variable being predicted, in this case __no__ (column 2) and __yes__ (column 3). The rows in this data.frame show how the errors change as the number of trees considered increases, from 1 (row 1) to ntree (last row). We can plot the OOB:

```{r echo=TRUE, include=TRUE}

plot(rf$err.rate[,1], t='l')

```
The out of bag error has already plateaued once ntree = 500 so 500, the default number of trees, is an appropriate number of trees for the random forest. 

```{r}
rf <- randomForest(class ~ ., data = dataset_question_2, ntree=500)

```
## A few quantifications of predictive power

In many occasions we would want to be able to summarize how well a random forest works in light of the data presented to it. In the sections above we looked at the OOB error, but there are 3 other commonly used measures - overall __accuracy__, __sensitivity__ (true positive rate) and __specificity__ (true negative rate).

Reporting sensitivity, specificity and accuracy: 

We use the confusion matrix to calculate the measures of predictive power.

```{r echo=TRUE, include=TRUE}

#extract confusion matrix
m<- rf$confusion
#remove the last column which is the error.class.  ?
m<- m[,-ncol(m)]
m

```

The overall __accuracy__ (ACC, as introduced in the lecture slides) is the number of correctly predicted entries divided by the sum of all entries.

```{r}

##calc accuracy (prop correctly classified)
ACC<- (m[1,1] + m[2,2])/sum(m)
ACC

```
The accuracy is 0.99 which is very high 

Calculating sensitivity / true positive rate. 
True positive rate is caluclated by the number of correctly predicted entires for 1 divided by the sum of all entries that are actually 1 / it is the proporiton of the units with a known positive condition for which the condition is actually positive. 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[2,2])/sum(m[2,])
TPR

```
The true positive rate is 99% which is very high 

Calculating specificity: 

The __true negative rate__ (TNR, as introduced in the lecture slides) is the number of correctly predicted entires for __no__ divided by the sum of all entries that are actually __no__.
In our case the true negative rate is calculated by the number of correctly predicted entries for 0 divided by the sum of all entries that are actually 0 

```{r}
##calc specificity (True Negative Rate)
TNR<- (m[1,1])/sum(m[1,])
TNR

```
The true negative rate is 99%. 

Repeating the same analysis using 200 data points from data.generator.2: 

```{r}
# Generate 200 data points
dataset2_question_2 <- data.generator.2(200)

# View the first few rows of the generated data
head(dataset2_question_2)

```

Running a random forest on this data: 

```{r}

##make sure that the variable we are predicting is a factor
##which tells the software we are doing a classification exercise
#we need to make sure the native speaker column is a factor variable! Converting it to a factor variable which is important otherwise random forest wont work 
dataset2_question_2$class<- factor(dataset2_question_2$class)

rf <- randomForest(class ~ ., data = dataset2_question_2)
rf

```

Exploring random forest parameters: 



```{r}

##force a large number of trees
rf <- randomForest(class ~ ., data = dataset2_question_2, ntree=1000)

##this is how the OOB error rates are extracted from the solution within "rf"
head(rf$err.rate)

```
Plotting the out of bag error rate for this larger number of trees: 

```{r}
plot(rf$err.rate[,1], t='l')

```

The out of bag error rate has plateaud once 500 trees have been used, so 500 is also an appropriate number of trees to use for this model. However the error rate for this random forest is higher than the error rate from the random forest generated from the first dataset, even for a very large number of trees

Put the number of trees back to 500: 

```{r}
rf <- randomForest(class ~ ., data = dataset2_question_2, ntree=500)

```

Calculating sensitivity, specificity and accuracy

First we need to extract the confusion matrix from this Random Forest model: 

```{r}
#extract confusion matrix
m<- rf$confusion
#remove the last column which is the error.class.  ?
m<- m[,-ncol(m)]
m

```

Calculating accuracy: 

The overall __accuracy__ (ACC, as introduced in the lecture slides) is the number of correctly predicted entries divided by the sum of all entries.

```{r}

##calc accuracy (prop correctly classified)
ACC<- (m[1,1] + m[2,2])/sum(m)
ACC

```

The accuracy is 99% 

Calculating sensitivity / True positive rate: 

True positive rate is caluclated by the number of correctly predicted entires for 1 divided by the sum of all entries that are actually 1 / it is the proporiton of the units with a known positive condition for which the condition is actually positive. 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[2,2])/sum(m[2,])
TPR

```

The true positive rate is 95%, which is slightly lower than the true positive rate in the alternative dataset generated by data.generator.1 

Calculating specificity / true negative rate. 

The __true negative rate__ (TNR, as introduced in the lecture slides) is the number of correctly predicted entires for __no__ divided by the sum of all entries that are actually __no__.
In our case the true negative rate is calculated by the number of correctly predicted entries for 0 divided by the sum of all entries that are actually 0 

```{r}
##calc specificity (True Negative Rate)
TNR<- (m[1,1])/sum(m[1,])
TNR
```

The true negative rate is 100%. This is higher than that of the dataset generated by data.generator.1.



Do you note any differences across datasets? Are these results expected based on your answer to Exercise 1?

The true positive rate for data.generator.2 is lower than for data.generator.1. Accuracy is the same, and the true negative rate is higher for data.generator.2 than for data.generator.1. 

Data.generator.1 is more likely to ahve a higher true positive rate as it uses all the variables, x1, x2 and x3 in order to classify objects into class 1 or class 0. It checks for three conditions, x1>0, and x2>0, and if these criteria are not met, if x3>0, class is also set to 1. It is therefore more likely to correclty identify positive instances leading to a higher true positive rate. Data.generator.2 assigns class label only if x1 and x3 are greater than 0 and does not take into account the value of x2. It is therefore more restrictive in identifying positive instances leading to a lower TPR as it potentially misses some true positive instances that data.generator.1 might have captured. However this leads to a higher true negative rate of data.generator.2 as it is less likely to misclassify an actual negative as a positive due to its stricter cireria for assigning positive labels. 


Question 3: 

```{r}
#in the future need to make an if loop for this function 
#install.packages("palmerpenguins")

library(palmerpenguins)
data("penguins")
head(penguins)

```

```{r}
#saving the raw data before I clean it 

write.csv(penguins, "data/penguins.raw.csv")

#Loading the data from the saved version: 

penguins.raw <- read.csv("data/penguins.raw.csv")

```

Cleaning the data: I want to remove the variables sex and year, and remove NAs. 

```{r}
library(dplyr)
library(tidyr)
#This needs to live in a separate folder called functions
clean_penguins <- function() {
  penguins.raw %>%
    drop_na()  # Removes rows with NA values
}

#this function takes the raw data and removes NA values and converts all the variables into factors 

#Using this function to clean my data 

penguins_cleaned <- clean_penguins()
write.csv(penguins_cleaned, "data/penguins_cleaned.csv")



#Making a function that subsets the data, removing variables that are irrelevent to us such as sex, year and island. 
#Could replace this with the code from the session, is it quicker? 

subset_columns <- function(data, column_names) {
  data %>%
    select(all_of(column_names))
}

#Using this function to remove variables sex, year and island from penguins_clean 

# Remove the variables sex, year, and island, and keep all other variables
penguins_selected_variables <- subset_columns(penguins_cleaned, c("species", "bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"))

head(penguins_selected_variables)

#do i need to write a csv for this? 

```

Creating histograms for each variable, stratifying by species. Each figure should contain 3 histograms, one per species 

NOTE THAT BEFORE WE RUN THE RANDOM FOREST WE NEED TO CONVERT THE VARIABLES TO FACTORS BUT THIS WONT WORK FOR THE HISTOGRAMS 
```{r}
#Loading the required package 
library(ggplot2)

# Create histograms stratified by species for bill length
ggplot(penguins_selected_variables, aes(x = bill_length_mm, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Bill Length by Species",
       x = "Bill Length (mm)", y = "Frequency") +
  theme_minimal()

```
Creating another set of histograms, this time for bill depth. 

```{r}
ggplot(penguins_selected_variables, aes(x = bill_depth_mm, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Bill Depth by Species",
       x = "Bill Depth (mm)", y = "Frequency") +
  theme_minimal()

```
Creating histograms for flipper length: 

```{r}
ggplot(penguins_selected_variables, aes(x = flipper_length_mm, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Flipper length by Species",
       x = "Flipper length (mm)", y = "Frequency") +
  theme_minimal()

```
Creating another set of histograms for body mass 

```{r}
ggplot(penguins_selected_variables, aes(x = body_mass_g, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Body mass by Species",
       x = "Body mass (g)", y = "Frequency") +
  theme_minimal()
```

Run a random forest on this data set, calibrate it by setting appropriate values: what is the random forest actually trying to find? We are trying to predict species solely based on the penguins physical attributes 



```{r}
#We need to ensure the species column is a factor variable: 
penguins_selected_variables$species<- factor(penguins_selected_variables$species)
#Running a random forest on my data set 
rf <- randomForest(species ~ ., data = penguins_selected_variables)
rf

```
Finding the value of ntree that is appropriate: 

```{r}
##force a large number of trees
rf <- randomForest(species ~ ., data = penguins_selected_variables, ntree=1000)
##this is how the OOB error rates are extracted from the solution within "rf"
head(rf$err.rate)
```
Plotting the OOB error rate against number of trees: 

```{r}
plot(rf$err.rate[,1], t='l')

```
The error rate is large for a single tree but plateaus as the number of trees increases. The error rate had already plateaud once 500 trees are reached therefore 500 is an appropriate number of trees to use, when mtree is the default. ]

Checking the effect of different mtree values: 

```{r}
mtry_values<- c(1,2,3,4) ##there are only 4 variables
ntree<- 1000 ##set to large
OOB_results<- c()
for(mt in mtry_values){
  rf <- randomForest(species ~ ., data = penguins_selected_variables, ntree=ntree, mtry=mt)
  ##OOB only
  newOOB<- rf$err.rate[,1] 
  ##save each OOB solution by row on a data.frame
  OOB_results<- rbind(OOB_results,newOOB)
}
#what is mt?
plot(OOB_results[1,], t='l', col=1, ylim=c(0,0.1))
for(mt in 2:length(mtry_values)){
  lines(OOB_results[mt,], col=mt)
}
legend("topright",legend=mtry_values,col =1:length(mtry_values) ,lty=1)

```

Get help with this! 
The OOB error is noisy and varies between runs but its variation is small. The out of bound error for a small value of mtree is large. The effect is visible but the OOB error plateaus once a value of 500 (the default) is reached. So for this dataset the default parameters of both ntree and mtree are ok to be used. 

```{r}
#rerunning the rf model with the default parameters 
rf <- randomForest(species ~ ., data = penguins_selected_variables)
rf

```
Finding the most important variables: 

## Variable (feature) importance

The random forest will quantify how important each variable (also commonly termed __feature_"__) was for the prediction. We can extract and plot this information from the forest.

```{r echo=TRUE, include=TRUE, fig.width=10, fig.height=4}
  
imp<- importance(rf) ##extracts a matrix with the importance information
print(imp)

varImpPlot(rf) ##plots the importance information

```

Bill length has the highest importance in predicting penguin species since it has the highest value for Mean decreae in Gini index. This is followed by flipper length, bill depth and body mass. 

Extracting the confusion matrix: 

```{r}
#extract confusion matrix
m<- rf$confusion
#remove the last column which is the error.class.  
m<- m[,-ncol(m)]
m


```
Calculating accuracy, sensitivity and specificity for each penguin species 

Adelie: 

Sensitivity / True positive rate: the number of predicted positives divided by how many there actually are 
```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[1,1])/sum(m[1,])
TPR

```
  
Specificity / true negative 

The number of correctly predicted nos divided by the actual number of negatives 



```{r}
TNR <- (m[2, 2] + m[3, 3]) / (sum(m[2, ]) + sum(m[3, ]))
TNR
```

Accuracy:
Calculating accuracy for each species 
 = number of correct predictions for that species / total number of predictions for that species 
 
```{r}
ACC <- (m[1,1]) / sum(m[,1])
ACC


```


Chinstrap: 
Sensitivity / True positive rate 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[2,2])/sum(m[2,])
TPR

```

Specificity / true negative rate 

```{r}
TNR <- (((m[1,1]) + (m[3,3]))) / (sum(m[1,]) + sum(m[3,]))
TNR

```

Accuracy 
 = number of correct predictions for that species / total number of predictions for that species 
 
```{r}
ACC <- (m[2,2]) / sum(m[,2])
ACC

```


Gentoo: 
Calculating True positive / Sensitivity 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[3,3])/sum(m[3,])
TPR

```

Calculating true negative / specificity 

```{r}
TNR <- (((m[1,1]) + (m[2,2])) / (sum(m[2,]) + sum(m[1,])))
TNR


```

Calculating accuracy 
```{r}
ACC <- (m[3,3]) / sum(m[,3])
ACC
```

Analysis: REDO THIS 

Are all species classified equally well? 
No, true positive, true negative rate and accuracy rates vary between species.
Chinstrap species are classified less accurately than Adelie. Adelie has a higher sensitivity, specificity and accuracy rate than chinstrap. Gentoo has the highest true negative and accuracy, and the highest true positive rate, so Gentoo are most accurately calssified. 




Bill length has the highest importance in predicting penguin species since it has the highest value for Mean decrease in Gini index. This is followed by flipper length, bill depth and body mass. 



```{r}
#Plotting Bill length against body mass 
library(ggplot2)
# Scatter plot of bill length against body mass
ggplot(penguins_selected_variables, aes(x = bill_length_mm, y = body_mass_g, color = species)) +
  geom_point() +
  labs(x = "Bill Length (mm)", y = "Body Mass (g)") +
  ggtitle("Scatter Plot of Bill Length vs. Body Mass")

```


Do these plots also help explain why some variables may be more important than others for classification?

This scatter plot shows how body mass of a species changes as bill length changes. The adelie, chinstrap and gentoo species cluster separately, showing clear boundaries in feature space, because bill length is an important feature in predicting species. 


```{r}
# Scatter plot of body mass against flipper length 
ggplot(penguins_selected_variables, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +
  geom_point() +
  labs(x = "Flipper length (mm)", y = "Body Mass (g)") +
  ggtitle("Scatter Plot of Flipper length vs. Body Mass")

```
This scatter plot shows how body mass varies with increasing flipper length. Although gentoo species still cluster separetely, Adelie and chinstrap cluster together because flipper length is a less important variable in differentiating between these two species.

```{r}
# Scatter plot of body mass against bill depth
ggplot(penguins_selected_variables, aes(x = bill_depth_mm, y = body_mass_g, color = species)) +
  geom_point() +
  labs(x = "Bill depth (mm)", y = "Body Mass (g)") +
  ggtitle("Scatter Plot of Bill depth vs. Body Mass")

```
Bill length is therefore the most important variable in predicting penguin species because it is the only variable for which there si a clear boundary between adelie and chinstrap in feature space. 

What if I plot bill length against a more important feature, against flipper length 

```{r}
#plotting bill length against flipper length 
ggplot(penguins_selected_variables, aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +
  geom_point() +
  labs(x = "Bill length (mm)", y = "Flipper length (mm)") +
  ggtitle("Scatter Bill length vs Flipper length")

```
Bill length and flipper length are more important in predicting which species an individual belongs to because when these variables are plotted against eachother, there are the clearest boundaries in space between all three species. 

Can decide which to keep later! 



Question 4: 

  

```{r}
# Load necessary libraries
library(palmerpenguins)
library(randomForest)
library(ggplot2)

  

```


```{r}
# Define values of n
n_values <- c(0, 1, 5, 10, 20, 50, 75, 100)
# Initialize a list to store the results
accuracy_results <- list()
# Iterate over each value of n
for (n in n_values) {  # Initialize a vector to store accuracies for this value of n
  accuracies <- numeric()    # Repeat the experiment M times
  M <- 50 
  for (i in 1:M) {    # Generate n noisy arrays    
    noisy_arrays <- lapply(1:n, function(j) rnorm(N, 0, 1))        # Create the augmented dataset    
    augmented_dataset <- penguins_selected_variables  # Create a new dataset    
    names(noisy_arrays) <- paste0("x", 1:length(noisy_arrays))    
    augmented_dataset <- cbind(augmented_dataset, noisy_arrays)        
    # Make sure the variable we are predicting is a factor    
    augmented_dataset$species <- factor(augmented_dataset$species)        
    # Train a random forest model on the augmented dataset    
    rf <- randomForest(species ~ ., data = augmented_dataset, ntree = 2000)        
    # Calculate accuracy and store it in accuracies vector    
    m <- rf$confusion    
    m <- m[,-ncol(m)]    
    ACC <- (m[1,1] + m[2,2] + m[3,3])/sum(m)    
    accuracies <- c(accuracies, ACC)  }   
  # Calculate mean accuracy for this value of n  
  mean_accuracy <- mean(accuracies)    
  # Store mean accuracy in accuracy_results list  
  accuracy_results[[as.character(n)]] <- mean_accuracy}
# Convert accuracy_results to a data frame
accuracy_df <- data.frame(  n = as.numeric(names(accuracy_results)),  mean_accuracy = unlist(accuracy_results))

```



```{r}
#Defining the values of n in each random array 
n_values <- c(0, 1, 5, 10, 20, 50, 75, 100) 

#Initialise a list to store the results of accuracy for each value of n 
accuracy_results <- list()

#Iterate what we want to do for each value of n
for (n in n_values) {
  #Initialise a vector to store the accuracies for this value of n 
  accuracies <- numeric()
  
  N <- nrow(penguins_selected_variables)  #assuming 'penguins_selected_variables' is your dataset. We are making a

  #repeat the experiment M times
  M <- 50 
  for (i in 1:M) {
    #generate these 9 noisy arrays
    x1 <- rnorm(N, 0, 1)
    x2 <- rnorm(N, 0, 1)
    x3 <- rnorm(N, 0, 1)
    x4 <- rnorm(N, 0, 1)
    x5 <- rnorm(N, 0, 1)
    x6 <- rnorm(N, 0, 1)
    x7 <- rnorm(N, 0, 1)
    x8 <- rnorm(N, 0, 1)
    x9 <- rnorm(N, 0, 1)
    
    # Appending this to my dataset: 
    # Assuming 'penguins_selected_variables' is your dataset
    augmented_dataset <- penguins_selected_variables  # Create a new dataset

    # Add the noisy variables to the new dataset
    augmented_dataset <- cbind(penguins_selected_variables, x1, x2, x3, x4, x5, x6, x7, x8, x9)

    # running a random forest on the augmented dataset
    ##make sure that the variable we are predicting is a factor
    augmented_dataset$species <- factor(augmented_dataset$species)
    rf <- randomForest(species ~ ., data = augmented_dataset, ntree = 2000)

    # calculating the accuracy of the random forest 
    m <- rf$confusion
    # remove the last column which is the error.class
    m <- m[,-ncol(m)]
      
    # Calculate accuracy
    ACC <- (m[1,1] + m[2,2] + m[3,3])/sum(m)
      
    # Store the accuracy for this repetition
    accuracies <- c(accuracies, ACC)
  }

  # Calculate the mean accuracy for this value of n
  mean_accuracy <- mean(accuracies)

  # Store the mean accuracy for this value of n in the accuracy_results list 
  accuracy_results[[as.character(n)]] <- mean_accuracy
}

# Convert accuracy_results to a data frame
#N values have to be treated as numeric in order to be able to plot the data 
accuracy_df <- data.frame(
  n = as.numeric(names(accuracy_results)),
  mean_accuracy = unlist(accuracy_results) #putting the list into a format that is readable in a data frame 
)




```

Plotting my data: 

```{r}
# Plot mean accuracy as a function of n
ggplot(accuracy_df, aes(x = n, y = mean_accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "n", y = "Mean Accuracy", title = "Mean Accuracy vs. Sample Size (n)")


```

```{r}
# Plot mean accuracy as a function of n
ggplot(accuracy_df, aes(x = n, y = mean_accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "n", y = "Mean Accuracy", title = "Mean Accuracy vs. Sample Size (n)") +
  scale_y_continuous(limits = c(0.9, 1))  # Adjust the limits of the y-axis



```

```{r}
# Define values of n
n_values <- c(0, 1, 5, 10, 20, 50, 75, 100)

# Initialize a list to store the results
accuracy_results <- list()

# Iterate over each value of n
for (n in n_values) {
  # Initialize a vector to store accuracies for this value of n
  accuracies <- numeric()
  
  # Repeat the experiment M times
  M <- 50
  for (i in 1:M) {
    # Generate noisy variables for this augmented dataset
    # Add noisy variables to the original dataset
    
    # Create the augmented dataset
    
    # Train a random forest model on the augmented dataset
    
    # Calculate accuracy and store it in accuracies vector
  }
  
  # Calculate mean accuracy for this value of n
  mean_accuracy <- mean(accuracies)
  
  # Store mean accuracy in accuracy_results list
  accuracy_results[[as.character(n)]] <- mean_accuracy
}

# Convert accuracy_results to a data frame and plot it
# ...



```

Corrected version where it makes multiple augmented data sets

```{r}

# Define values of n
n_values <- c(0, 1, 5, 10, 20, 50, 75, 100)

# Initialize a list to store the results for accuracy calculations for each data frame 
accuracy_results <- list()

#iterate over each value of n
for (n in n_values) {
  #initialize a vector to store accuracies for this value of n
  accuracies <- numeric()
  
  #Repeat the experiment M times where M is 50 
  M <- 50
  for (i in 1:M) { #for every experiment out of M experiments 
    # Generate n noisy arrays where n is n_values, drawn randomly from the normal distribution
    noisy_arrays <- lapply(1:n, function(j) rnorm(N, 0, 1)) #where N is the length of the penguins_selected_variables_dataset
    
    #Augmenting our new noisy arrays to the penguins_selected_variables dataset creating multiple new datasets 
    augmented_dataset <- penguins_selected_variables  # Create a new dataset that is identical to my previous dataset 
    names(noisy_arrays) <- paste0("x", 1:length(noisy_arrays)) #naming the columns from 1 to the number of noisy arrays thatI have
    augmented_dataset <- cbind(augmented_dataset, noisy_arrays) #binding the noisy arrays I have made to my new augmented dataset 
    
    # Make sure the variable we are predicting is a factor so the random forest works 
    augmented_dataset$species <- factor(augmented_dataset$species)
    
    # Train a random forest model on the augmented dataset
    rf <- randomForest(species ~ ., data = augmented_dataset, ntree = 2000)
    
    # Calculate accuracy and store it in accuracies vector
    m <- rf$confusion
    m <- m[,-ncol(m)]
    ACC <- (m[1,1] + m[2,2] + m[3,3])/sum(m)
    accuracies <- c(accuracies, ACC)
  }
  
  #This for loop will be completed 50 times for each value of n in n_values 
  
  # Calculate mean accuracy for this value of n
  mean_accuracy <- mean(accuracies)
  
  # Store mean accuracy in accuracy_results list
  accuracy_results[[as.character(n)]] <- mean_accuracy #because a list can only store character values 
}


# Convert accuracy_results to a data frame
accuracy_df <- data.frame(
  n = as.numeric(names(accuracy_results)),
  mean_accuracy = unlist(accuracy_results)
)


```

Plotting this data

```{r}

library(ggplot2)

# Plot mean accuracy as a function of n
ggplot(accuracy_df, aes(x = n, y = mean_accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "n", y = "Mean Accuracy", title = "Mean Accuracy vs. Sample Size (n)")


```

Question 5: 

Loading the data:

```{r}

covertype_data <- read_csv( "/Users/cissi.user.2/Documents/Computer coding/Random forest assignment/Data/covertype.csv")

head(covertype_data)
```
Creating two smaller data sets, data_train and data_test of sizes Ntrain = 1000 and Ntest = 1000 with data points selected at random from the full data set. Data_train and data_test cannot share any data points.



```{r}
# Set the sizes for training and testing sets
Ntrain <- 10000
Ntest <- 1000

# Generate indices for all rows
total_rows <- nrow(covertype_data)
all_indices <- 1:total_rows

# Sample indices for data_train
train_indices <- sample(all_indices, Ntrain, replace = FALSE)

# Exclude train_indices from the pool of indices
remaining_indices <- setdiff(all_indices, train_indices)

# Sample indices for data_test from the remaining indices
test_indices <- sample(remaining_indices, Ntest, replace = FALSE)

# Create data_train and data_test using the sampled indices
data_train <- covertype_data[train_indices, ]
data_test <- covertype_data[test_indices, ]

# Visualize the data
data_train
data_test

```
Another way of generating data_train and data_test ensuring factor levels are consistent between them 

```{r}
# Set seed for reproducibility
set.seed(123)

# Define sizes
Ntrain <- 10000
Ntest <- 1000

# Sample indices for data_train
train_indices <- sample(nrow(covertype_data), Ntrain, replace = FALSE)

# Sample indices for data_test
all_indices <- 1:nrow(covertype_data)
test_indices <- sample(setdiff(all_indices, train_indices), Ntest, replace = FALSE)

# Create data_train and data_test
data_train <- covertype_data[train_indices, ]
data_test <- covertype_data[test_indices, ]

# Check and adjust factor levels if needed, with explicit adjustment for Soil_Type
for (var in names(data_test)) {
  if (is.factor(data_test[[var]])) {
    levels_train <- levels(data_train[[var]])
    levels_test <- levels(data_test[[var]])
    if (!identical(levels_train, levels_test)) {
      if (var == "Soil_Type") {
        # Adjust factor levels for Soil_Type
        common_levels <- intersect(levels_train, levels_test)
        data_test[[var]] <- factor(data_test[[var]], levels = common_levels)
      } else {
        # Adjust factor levels for other variables
        warning(paste("Factor levels for", var, "in data_test differ from data_train. Adjusting..."))
        data_test[[var]] <- factor(data_test[[var]], levels = levels_train)
      }
    }
  }
}

# Check and ensure consistency in column names and variable types
# Adjust column names if needed
if (!identical(names(data_train), names(data_test))) {
  names(data_test) <- names(data_train)
}

# Ensure consistency in variable types
for (var in names(data_test)) {
  if (class(data_test[[var]]) != class(data_train[[var]])) {
    data_test[[var]] <- as.data.frame(data_test[[var]], col.names = class(data_train[[var]]))
  }
}

# Check and ensure consistency in factor levels again
if (!identical(levels(data_train), levels(data_test))) {
  warning("Factor levels in data_train and data_test are not fully consistent.")
}

data_train
data_test


```
```{r}
# Generate indices for all rows
total_rows <- nrow(covertype_data)
all_indices <- 1:total_rows

# Sample indices for data_train
train_indices <- sample(all_indices, Ntrain, replace = FALSE)

# Exclude train_indices from the pool of indices
remaining_indices <- setdiff(all_indices, train_indices)

# Sample indices for data_test from the remaining indices
test_indices <- sample(remaining_indices, Ntest, replace = FALSE)

# Create data_train and data_test using the sampled indices
data_train <- covertype_data[train_indices, ]
data_test <- covertype_data[test_indices, ]

# Adjust data types in data_test to match data_train
for (var in names(data_train)) {
  if (class(data_train[[var]]) != class(data_test[[var]])) {
    data_test[[var]] <- as(data_test[[var]], class(data_train[[var]]))
  }
}

# Check data types
identical_dtypes <- identical(sapply(data_train, class), sapply(data_test, class))
if (!identical_dtypes) {
  stop("Data types in data_train and data_test do not match.")
}

data_train
data_test



```


```{r}
# Find the columns representing Wilderness_AreaX
wilderness_cols <- grep("Wilderness_Area", names(data_train))

# Get the column names without the Wilderness_Area prefix
wilderness_areas <- gsub("Wilderness_Area", "", names(data_train[wilderness_cols]))

# Find the column index where Wilderness_AreaX = 1 for each row
wilderness_area_index <- apply(data_train[, wilderness_cols], 1, function(x) which(x == 1))

# Extract the corresponding Wilderness_Area value
data_train$Wilderness_Area <- wilderness_areas[wilderness_area_index]

# Convert Wilderness_Area to a factor
data_train$Wilderness_Area <- factor(data_train$Wilderness_Area)

# Remove the original one-hot-encoded Wilderness_AreaX columns
data_train <- data_train[, -wilderness_cols]

#repeating this for the soil columns 

# Find the columns representing Soil_TypeX
soil_cols <- grep("Soil_Type", names(data_train))

# Get the column names without the Soil_Type prefix
soil_types <- gsub("Soil_Type", "", names(data_train[soil_cols]))

# Find the column index where Soil_TypeX = 1 for each row
soil_type_index <- apply(data_train[, soil_cols], 1, function(x) which(x == 1))

# Extract the corresponding Soil_Type value
data_train$Soil_Type <- soil_types[soil_type_index]

# Convert Soil_Type to a factor
data_train$Soil_Type <- factor(data_train$Soil_Type)

# Remove the original one-hot-encoded Soil_TypeX columns
data_train <- data_train[, -soil_cols]

head(data_train)


```

Running a random forest on data_train with the default parameters: 


```{r}
#Converting class to a factor so the random forest can work 
data_train$class <- factor(data_train$class)
#Running the random forest model on my data_train 
rf <- randomForest(class ~ ., data = data_train)
rf


```

Reporting variable importance: 

```{r}
imp<- importance(rf) ##extracts a matrix with the importance information
print(imp)

varImpPlot(rf) ##plots the importance information

```

Extracting the confusion matrix: 


```{r}
str(rf$confusion)


```

```{r}
#extract confusion matrix
m<- rf$confusion
#remove the last column which is the error.class.  ?
m<- m[,-ncol(m)]
m



```
Class 1: 

Accuracy: 

Accuracy 
 = number of correct predictions for that species / total number of predictions for that species 
 

```{r}
ACC <- (m[1,1]) / sum(m[,1])
ACC

```


Sensitivity / True positive 

The number of correctly predicted positives divided by number of actual positives 

```{r}
TPR <- m[1, 1] / sum(m[1, ])
TPR

```

Specificity / True negative rate

Have calculated this wrong above! 

The __true negative rate__ (TNR, as introduced in the lecture slides) is the number of correctly predicted entires for __no__ divided by the sum of all entries that are actually __no__.


```{r}


TNR <- ((m[2,2]) + (m[3,3]) + (m[4,4]) + (m[5,5]) + (m[6,6]) + (m[7,7])) / ((sum(m[2,]) + sum(m[3,]) + sum(m[4,]) + sum(m[5,]) + sum(m[6,]) + sum(m[7,])))
TNR



```


Trying to get a loop to do this for all the classes in R: 

```{r}
# Assuming m is your confusion matrix
m  

# Number of classes
num_classes <- nrow(m)

# Initialize vectors to store results
accuracy <- numeric(num_classes)
sensitivity <- numeric(num_classes)
specificity <- numeric(num_classes)

# Calculate metrics for each class
for (i in 1:num_classes) {
  # Accuracy
  accuracy[i] <- m[i, i] / sum(m[, i])
  
  # Sensitivity / True positive rate
  sensitivity[i] <- m[i, i] / sum(m[i, ])
  
  # True negative rate / Specificity
  true_negatives <- sum(diag(m)) - m[i, i]  # Subtract true positives from total true positives and true negatives
  total_negatives <- sum(m) - sum(m[i, ])   # Subtract true positives and false positives from total
  specificity[i] <- true_negatives / total_negatives
}

# Print or return the results
print(accuracy)
print(sensitivity)
print(specificity)

# Create a data frame with class and metrics
results_df <- data.frame(
  class = 1:num_classes,
  accuracy = accuracy,
  sensitivity = sensitivity,
  specificity = specificity
)

# Print or return the data frame
print(results_df)



```

Analyse this part of the results later 

Final part of question 5: 



Making the same observations I did on data_test as I did on data_train 

```{r}
# Find the columns representing Wilderness_AreaX
wilderness_cols <- grep("Wilderness_Area", names(data_test))

# Get the column names without the Wilderness_Area prefix
wilderness_areas <- gsub("Wilderness_Area", "", names(data_test[wilderness_cols]))

# Find the column index where Wilderness_AreaX = 1 for each row
wilderness_area_index <- apply(data_test[, wilderness_cols], 1, function(x) which(x == 1))

# Extract the corresponding Wilderness_Area value
data_test$Wilderness_Area <- wilderness_areas[wilderness_area_index]

# Convert Wilderness_Area to a factor
data_test$Wilderness_Area <- factor(data_test$Wilderness_Area)

# Remove the original one-hot-encoded Wilderness_AreaX columns
data_test <- data_test[, -wilderness_cols]

#repeating this for the soil columns 

# Find the columns representing Soil_TypeX
soil_cols <- grep("Soil_Type", names(data_test))

# Get the column names without the Soil_Type prefix
soil_types <- gsub("Soil_Type", "", names(data_test[soil_cols]))

# Find the column index where Soil_TypeX = 1 for each row
soil_type_index <- apply(data_test[, soil_cols], 1, function(x) which(x == 1))

# Extract the corresponding Soil_Type value
data_test$Soil_Type <- soil_types[soil_type_index]

# Convert Soil_Type to a factor
data_test$Soil_Type <- factor(data_test$Soil_Type)

# Remove the original one-hot-encoded Soil_TypeX columns
data_test <- data_test[, -soil_cols]

head(data_test)



```





pred_test <- predict(model, newdata = data_test, type= "class")


```{r}

# Check column names
identical_colnames <- identical(colnames(data_train), colnames(data_test))
if (!identical_colnames) {
  stop("Column names in data_train and data_test do not match.")
}

# Check data types
identical_dtypes <- identical(sapply(data_train, class), sapply(data_test, class))
if (!identical_dtypes) {
  stop("Data types in data_train and data_test do not match.")
}

# Check and adjust factor levels if needed
for (var in names(data_test)) {
  if (is.factor(data_test[[var]])) {
    levels_train <- levels(data_train[[var]])
    levels_test <- levels(data_test[[var]])
    if (!identical(levels_train, levels_test)) {
      warning(paste("Factor levels for", var, "in data_test differ from data_train. Adjusting..."))
      data_test[[var]] <- factor(data_test[[var]], levels = levels_train)
    }
  }
}

# Now, make predictions using the random forest model
predictions <- predict(rf, newdata = data_test, type = "class")
print(predictions)



```

```{r}
#adjusting factor levels in data_train and data_test 
# Check and adjust factor levels if needed, with explicit adjustment for Soil_Type
for (var in names(data_test)) {
  if (is.factor(data_test[[var]])) {
    levels_train <- levels(data_train[[var]])
    levels_test <- levels(data_test[[var]])
    if (!identical(levels_train, levels_test)) {
      if (var == "Soil_Type") {
        # Adjust factor levels for Soil_Type
        common_levels <- intersect(levels_train, levels_test)
        data_test[[var]] <- factor(data_test[[var]], levels = common_levels)
      } else {
        # Adjust factor levels for other variables
        warning(paste("Factor levels for", var, "in data_test differ from data_train. Adjusting..."))
        data_test[[var]] <- factor(data_test[[var]], levels = levels_train)
      }
    }
  }
}


```

Running the random forest prediction: 

```{r}
#Converting class to a factor so the random forest can work 
data_test$class <- factor(data_test$class)

#making predictions on the test data using the random forest I just made 
predictions <- predict(rf, newdata = data_test, type = "class")
#printing the predictions 
print(predictions)

```



Go back and look at how I made my data_train and data_test, this might help.

```{r}
# Identify categorical variables in your datasets (assuming `data_train` and `data_test` are data frames)
categorical_vars <- sapply(data_train, is.factor)

# Retrieve and compare factor levels for each categorical variable
for (var in names(data_train)[categorical_vars]) {
  cat("Variable:", var, "\n")
  cat("Levels in data_train:", levels(data_train[[var]]), "\n")
  cat("Levels in data_test:", levels(data_test[[var]]), "\n")
  cat("\n")
}


```

There is an issue here: 

There is a level 25 in data_test that is not found in data_trait in soil type 

There is a level 28 in data_test that is not found in data_train 

There si a level 3 in data_Test not found in data_train 

There is a level 37 in data_Test not found in data_train 

There is a level 5 in data_test not found in data train 


New code to generate data_train and data_test that shouldn't cause these issues 

```{r}
# Identify categorical variables in your dataset (assuming `covertype_data` is a data frame)
categorical_vars <- sapply(covertype_data, is.factor)

# Get unique factor levels for each categorical variable
unique_levels <- lapply(covertype_data[, categorical_vars], levels)
unique_levels_combined <- unique(unlist(unique_levels))

# Split the unique levels into training and testing sets
set.seed(123)  # Set seed for reproducibility
train_levels <- sample(unique_levels_combined, floor(0.8 * length(unique_levels_combined)))
test_levels <- setdiff(unique_levels_combined, train_levels)

# Split the dataset into training and testing sets based on the selected levels
data_train <- covertype_data[sapply(covertype_data[, categorical_vars], function(x) any(x %in% train_levels)), ]
data_test <- covertype_data[sapply(covertype_data[, categorical_vars], function(x) any(x %in% test_levels)), ]

# Ensure consistent factor levels in data_train and data_test
for (var in names(data_train)[categorical_vars]) {
  levels(data_train[[var]]) <- union(levels(data_train[[var]]), levels(data_test[[var]]))
  levels(data_test[[var]]) <- levels(data_train[[var]])
}

```

```{r}
# Set seed for reproducibility
set.seed(123)

# Sample indexes for training and testing sets
train_indexes <- sample(nrow(covertype_data), 10000)
test_indexes <- sample(setdiff(1:nrow(covertype_data), train_indexes), 1000)

# Create training and testing datasets
data_train <- covertype_data[train_indexes, ]
data_test <- covertype_data[test_indexes, ]

# Ensure consistent factor levels in both datasets
for (var in names(data_train)) {
  if (is.factor(data_train[[var]])) {
    levels_train <- levels(data_train[[var]])
    levels_test <- levels(data_test[[var]])
    levels_combined <- union(levels_train, levels_test)
    data_train[[var]] <- factor(data_train[[var]], levels = levels_combined)
    data_test[[var]] <- factor(data_test[[var]], levels = levels_combined)
  }
}


```

```{r}
# Set the sizes for training and testing sets
Ntrain <- 10000
Ntest <- 1000

# Generate indices for all rows
total_rows <- nrow(covertype_data)
all_indices <- 1:total_rows

# Sample indices for data_train
train_indices <- sample(all_indices, Ntrain, replace = FALSE)

# Exclude train_indices from the pool of indices
remaining_indices <- setdiff(all_indices, train_indices)

# Sample indices for data_test from the remaining indices
test_indices <- sample(remaining_indices, Ntest, replace = FALSE)

# Create data_train and data_test using the sampled indices
data_train <- covertype_data[train_indices, ]
data_test <- covertype_data[test_indices, ]

# Visualize the data
data_train
data_test


```



Use this to regenerate data_train and data_test and run all the code I have above on it to do question 5.



```