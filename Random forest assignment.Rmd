Analysing the functions: 

Explain how each function decides the value of y based on x1, x2 and x3. Do you think these data-generating mechanisms are consistent with the structure of a decision tree? Why?


```{r}
data.generator.1 <- function(N) { #we are making a funciton called data generator 
  x1 <- rnorm(N,0,1) #predictor 1 #we are making a variable called x1, which is used to decide y: we are drawing the value of x from a normal distribution? 
  x2 <- rnorm(N,0,1) #predictor 2: same as above 
  x3 <- rnorm(N,0,1) #predictor 3 
  
  class <- rep(0, each=N) #outcome (to be filled below)  making a new object called class. We decide what class each i is in based on the value of x1, x2 and x3 
  #assigns a class to each data point
  for ( i in 1:N ) { #for every i / object in 1/N that it samples 
    if ( x1[i] >0 ) { #if the variable of x1 of i is greater than 0
      if (x2[i] >0 ) { #and if the variable of x2 of i is greater than 0
        class[i] = 1 #put it in class 1 
      }
      else
        class[i] = 0 #if this condition is not met, put it in class 0 
    }
    else #but 
      if ( x3[i] > 0 ) { #if the value of x3 of i is greater than 0 
        class[i] = 1 #put it in class 1 
      }
    else
      class[i] = 0 #if not, put it in class 0 
  }
  class <- factor(class) #making an object called class where being in class 0 or 1 is a factor variable, this is y 
  data <- data.frame(x1, x2, x3, class) #making a data frame which is made up of 4 variables, values of x1, x2 and x3, and what class you are in 
  data
  
}
```

data.generator.1: 

The function data.generator.1 is drawing objects from a dataset of N number of objects. For each object i in N, it then makes three continuous variables caled x1, x2 and x3. The values of x1 x2 and x3 assigned to each value of i are taken from a random sample of a normal distribution. The function then makes a second variable for each object i in N called class, which will be the y variable. It initially sets the value of class for each object i to 0. 

The function then has a for loop which is used to decide whether the value of class of the data point i in N is 0 or 1, depending on the values of x1, x2 and x3. The for loop states that if the value of x1 >0, the value of x2 should be assessed. If the value of X2 > 0, class is set to 1. The 'else' part of the for loop states that if x1 is less or equal to 0, the value of x3 should be assessed. If x3 > 0, class is set to 1. If x3 is less or equal to 0, class remains at 0. 

Do you think these data-generating mechanisms are consistent with the structure of a decision tree? Why?

Decision trees define thresholds around observed values of variables which results in divisions of data. They use recursive binary splitting processes, meaning a series of decisions are taken around whether the values of the variables fall above or below a certain value until the end nodes of the decision are pure. This data generating mechanism is consistent with the structure of a decision tree. The function uses the value of three variables, x1, x2, and x3 and assigns the value of the categorical variable y based on the outcome of whether the values of x1, x2 and x3 fall above or below a fixed value, 0. The end result of the function is that the data is divided into fixed groups where the nodes are pure, as the value of y (class) has either been assigned to 0 or 1.


data.generator.2 

```{r}

data.generator.2 <- function( N ) { #making a function defining characteristics about N 
  x1 <- rnorm(N,0,1) #predictor 1: made a variable called x1, we assign values of x1 by drawing randomly from the normal distribution 
  x2 <- rnorm(N,0,1) #predictor 2
  x3 <- rnorm(N,0,1) #predictor 3 
  class <- rep(0, each=N) #outcome (to be fulfilled below): we have made the variable class, right now every data point i in N is class 0 but the true outcome of class is decided by the outcome of the foor loop below: 
  #assigns a class to each data point 
  for ( i in 1:N ) { #for every data point i in N 
    if ( x1[i] >0 ) { #if x1 value is greater than 0
      if ( x3[i] > 0 ) { #then assess the value of x3. If x3 > 0, it is class 1 
        class[i] = 1
      }
      else
        class[i] = 0 #if this condition is not fulfilled, so if value of x1 is not greater than 0 or if the value of x3 is not greater than 0, put it in class 0.
    }
    else
      class[i] = 0 #if none of these conditions are met, put it in class 0 
  }
  class <- factor(class)
  data <- data.frame(x1, x2, x3, class)
  data
}

```

Explain how each function decides the value of y based on x1, x2 and x3.

In the same way as the function data.generator.1, data.generator.2 draws objects from a dataset of N number of objects. For each object i in N, it then makes three continuous variables caled x1, x2 and x3. The values of x1 x2 and x3 assigned to each value of i are taken from a random sample of a normal distribution. The function then makes a second variable for each object i in N called class, which will be the y variable. It initially sets the value of class for each object i to 0. 

The function then has a for loop which is used to decides whether the value of class of the data point i in N is 0 or 1, depending on the values of x1 and x3. The for loop states that for each data point in N, if the value of x1 is greater than 0, the value of x3 should be assessed. If the value of x3 is greater than 0, class should be assigned as 1. If the value of x1 or x3 is less than 0, class should be assigned as 0. If a condition other than these outcomes is met (such as any value of x2), class should still be assigned as 0. 

Do you think these data-generating mechanisms are consistent with the structure of a decision tree? Why? 

Yes this data generating mechanism is consistent with the structure of a decision tree. Although it only uses information about two out of three of the variables, x1 and x3, the tree still uses a recursive binary splitting process. This means that a series of decisions is taken based on whether the values of x1 and x3 fall above or below a certain value, 0, until the data are divided into fixed groups where the end nodes of the decision tree are pure. This means that the outcome of the decision tree is binary; any data point i in N is either in class 1 or class 0. Using some but not all of the possible variables to decide the final outcome in a decision tree is common practice in Random Forest. Random forest uses many decision trees on the same set of data which use different variables in order to avoid overfitting. 


Question 2: 

```{r}
# List of required packages
required_packages <- c("cowplot", "randomForest", "pROC", "reprtree", "caret", "palmerpenguins", "dplyr", "tidyr", "ggplot2", "readr")

# Check if each required package is installed
for (pkg in required_packages) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    # If not installed, install it
    install.packages(pkg)
  }
  # Load the package
  library(pkg, character.only = TRUE)
}

#The package __reprtree__ can be installed using: 

#install.packages("devtools")
#devtools::install_github('araastat/reprtree')



```



```{r}
# Generate 200 data points using data.generator.1 
dataset_question_2 <- data.generator.1(200)

# View the first few rows of the generated data
head(dataset_question_2)


```

Running a random forest model to predict y given the values of x1, x2 and x3 that we have generated above. We are feeding the data and the outcomes to our model so that our model learns how to predict the outcomes from new data based on this. 

```{r}

##make sure that the variable we are predicting is a factor
##which tells the software we are doing a classification exercise
#we need to make sure the native speaker column is a factor variable! Converting it to a factor variable which is important otherwise random forest wont work 
dataset_question_2$class<- factor(dataset_question_2$class)
# Set a seed for reproducibility
set.seed(123) #ensures the random numbers generated by R in the random forest algorithm are reproducable
rf <- randomForest(class ~ ., data = dataset_question_2)
rf

```
Exploring random forest parameters: 


```{r}
# Set a seed for reproducibility
set.seed(123)
##force a large number of trees onto our random forest
rf <- randomForest(class ~ ., data = dataset_question_2, ntree=1000)

##this is how the OOB error rates are extracted from the solution within "rf"
head(rf$err.rate)
```
This output of rf$err.rate estimates the error of the random forest model for the entire dataset which is shown in OOB, column 1. The other two columns are the two available values of the variable being predicted which in this case are NO, (0, column 2) and YES, (1, column 3). 

The rows show how the errors change as the number of trees considered increases, from 1, which is row 1, to ntree which is the last row, which in this case would be row 1000. 


We can plot the OOB:

We can plot the OOB. We have plotted the OOB error rate on the y axis against the number of trees on the x axis. 

```{r echo=TRUE, include=TRUE}

plot(rf$err.rate[,1], t='l')

```
The out of bag error has already plateaued once ntree = 500 so 500, the default number of trees, is an appropriate number of trees for the random forest. 

```{r}
# Set a seed for reproducibility
set.seed(123)
rf <- randomForest(class ~ ., data = dataset_question_2, ntree=500) #setting the number of trees in the random forest back to the default value, 500. 

```


Reporting sensitivity, specificity and accuracy: 

We use the confusion matrix to calculate the measures of predictive power, sensitivity, specificity and accuracy.

```{r echo=TRUE, include=TRUE}

#extract confusion matrix from my random forest model 
m<- rf$confusion
#remove the last column which is the error.class. 
m<- m[,-ncol(m)]
m

```

The overall accuracy is the number of correctly predicted entries divided by the sum of all entries.

```{r}

##calc accuracy (prop correctly classified)
ACC<- (m[1,1] + m[2,2])/sum(m)
ACC

```
The accuracy is 0.98 which is very high 

Calculating sensitivity / true positive rate. 
True positive rate is caluclated by the number of correctly predicted entires for 1 divided by the sum of all entries that are actually 1 / it is the proporiton of the units with a known positive condition for which the condition is actually positive. 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[2,2])/sum(m[2,])
TPR

```
The true positive rate is 97% which is very high 

Calculating specificity: 

The __true negative rate__ (TNR, as introduced in the lecture slides) is the number of correctly predicted entires for __no__ divided by the sum of all entries that are actually __no__.
In our case the true negative rate is calculated by the number of correctly predicted entries for 0 divided by the sum of all entries that are actually 0 

```{r}
##calc specificity (True Negative Rate)
TNR<- (m[1,1])/sum(m[1,])
TNR

```
The true negative rate is 99%. 

Repeating the same analysis using 200 data points from data.generator.2: 

```{r}
# Generate 200 data points using data.generator.2
dataset2_question_2 <- data.generator.2(200)

# View the first 6 rows of the generated data
head(dataset2_question_2)

```

Running a random forest on this data: 

```{r}

##make sure that the variable we are predicting is a factor
##which tells the software we are doing a classification exercise
#we need to make sure the native speaker column is a factor variable! Converting it to a factor variable which is important otherwise random forest wont work 
dataset2_question_2$class<- factor(dataset2_question_2$class)
# Set a seed for reproducibility
set.seed(123)
rf <- randomForest(class ~ ., data = dataset2_question_2)
rf

```

Exploring random forest parameters: 



```{r}

##force a large number of trees
# Set a seed for reproducibility
set.seed(456)
rf <- randomForest(class ~ ., data = dataset2_question_2, ntree=1000)

##this is how the OOB error rates are extracted from the solution within "rf"
head(rf$err.rate)

```
Plotting the out of bag error rate for this larger number of trees: 

```{r}
plot(rf$err.rate[,1], t='l')

```

The out of bag error rate has plateaud once 500 trees have been used, so 500 is also an appropriate number of trees to use for this model. However the error rate for this random forest is higher than the error rate from the random forest generated from the first dataset, even for a very large number of trees


```{r}
# Set a seed for reproducibility
set.seed(456)
#setting the number of trees in the model back to 500 which is the default number of trees 
rf <- randomForest(class ~ ., data = dataset2_question_2, ntree=500)

```

Calculating sensitivity, specificity and accuracy


Extracting the confusion matrix from the random forest model 

```{r}
#extract confusion matrix
m<- rf$confusion
#remove the last column which is the error.class.  ?
m<- m[,-ncol(m)]
m

```

I have extracted the confusion matrix from my random forest model. Rows denote the actual number of cases for class 0 and 1. Columns denote the predicted number of cases for classes 0 and 1. The correctly predicted number of cases is denoted by the 'diagonal' line across the matrix, ie m[1,1] and m[2,2] for class 0 and 1 respectively.  

Calculating accuracy: 

The overall accuracy is the number of correctly predicted entries divided by the sum of all entries.

```{r}

##calc accuracy (prop correctly classified)
ACC<- (m[1,1] + m[2,2])/sum(m)
ACC

```

The accuracy is 100%

Calculating sensitivity / True positive rate: 

True positive rate is calculated as the number of correctly predicted entries for 1 divided by the sum of all entries that are actually 1.It is the proportion of the units with a known positive condition for which the condition is actually positive. 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[2,2])/sum(m[2,])
TPR

```

The true positive rate is 95%, which is slightly lower than the true positive rate in the alternative dataset generated by data.generator.1 

Calculating specificity / true negative rate. 

The true negative rate is the number of correctly predicted entires for no divided by the sum of all entries that are actually no.
In our case the true negative rate is calculated by the number of correctly predicted entries for 0 divided by the sum of all entries that are actually 0 

```{r}
##calc specificity (True Negative Rate)
TNR<- (m[1,1])/sum(m[1,])
TNR
```

The true negative rate is 100%. This is higher than that of the dataset generated by data.generator.1.

Do you note any differences across datasets? Are these results expected based on your answer to Exercise 1?

 The true positive rate, accuracy and true negative rate is higher for data.generator.2 than for data.generator.1 

Data.generator 1 uses all the variables, x1, x2 and x3 to classify objects into class 1 or class 0, checking for three conditions, x1 >0, x2>0 and if these criteria are not met, if x3>0, in which case class is set to 1. Whereas data.generator.2 assigns a class label only if x1 and x3 are greater than 0 and does not take into account the value of x2. It therefore uses fewer variables in order to assign the value of y. The additional variable, x3 might not actually have good predictive power. The random forest model takes a subset of features in order to build individual decision trees meaning that the variables that are actually important could be missed out. This is known as overfitting, and explains the reduced accuracy, true positive rate and true negative of data.generator.1 in comparison to data.generator.2.




Question 3: 

```{r}
#Loading the data on penguins from the package "palmerpenguins" which was installed and loaded above. 
data("penguins")
head(penguins)

```

```{r}
#saving the raw data before I clean it 

write.csv(penguins, "data/penguins.raw.csv")

#Loading the data from the saved version: 

penguins.raw <- read.csv("data/penguins.raw.csv")

```




```{r, echo = FALSE}
#I am performing a number of steps which involve removing the variables sex and year which are not physical attributes and therefore will not be included in this analysis. I will also remove any rows containing NA values. 

#Making a function that removes the NA values from my penguins.raw
clean_penguins <- function() {
  penguins.raw %>%
    drop_na()  #Removes rows with NA values
}

#this function takes the raw data and removes NA values and converts all the variables into factors 

#Using this function to clean my data 

penguins_cleaned <- clean_penguins() #calling the function 
write.csv(penguins_cleaned, "data/penguins_cleaned.csv") #saving my cleaned data as a .csv 

#penguins_cleaned does not have any NA values in it 

#Making another function that subsets the data, removing variables that are irrelevant to us such as sex, year and island. 

subset_columns <- function(data, column_names) {
  data %>%
    select(all_of(column_names))
}

#Using this function to remove variables sex, year and island from penguins_clean 

# Remove the variables sex, year, and island, and keep all other variables
penguins_selected_variables <- subset_columns(penguins_cleaned, c("species", "bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g"))
#I used this funciton to only select variables relevent to physical attributes of the penguins to be included in my new data frame penguins_selected_variables

head(penguins_selected_variables)

#penguins_selected variables does not have any NA values and does not contain the columns of sex, year and island that are not relevant to physical attributes of the penguins

```

Creating histograms for each variable, stratifying by species. Each figure will contain 3 histograms, one per species 



```{r}


# Create histograms stratified by species for bill length
ggplot(penguins_selected_variables, aes(x = bill_length_mm, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Bill Length by Species",
       x = "Bill Length (mm)", y = "Frequency") +
  theme_minimal()

```
The histogram above shows the frequency distribution of bill length for each species 

I am creating a set of histograms showing how the frequency distribution of bill depth for each species:

```{r}
ggplot(penguins_selected_variables, aes(x = bill_depth_mm, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Bill Depth by Species",
       x = "Bill Depth (mm)", y = "Frequency") +
  theme_minimal()

```
Creating histograms for flipper length: 

```{r}
ggplot(penguins_selected_variables, aes(x = flipper_length_mm, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Flipper length by Species",
       x = "Flipper length (mm)", y = "Frequency") +
  theme_minimal()

```
The histogram above shows the frequency distribution of flipper length for each species 

Creating another set of histograms for body mass 

```{r}
ggplot(penguins_selected_variables, aes(x = body_mass_g, fill = species)) +
  geom_histogram(bins = 30, color = "black", alpha = 0.6) +
  facet_wrap(~species, scales = "free") +
  labs(title = "Distribution of Body mass by Species",
       x = "Body mass (g)", y = "Frequency") +
  theme_minimal()
```
This histogram shows the frequency distribution of body mass for each species 


I will next run a random forest on this data set which will predict species solely based on the physical attributes of the penguins. I will calibrate it by setting the appropriate values for ntree and mtree. 
```{r}
#We need to ensure the species column is a factor variable otherwise the random forest model will not work 
penguins_selected_variables$species<- factor(penguins_selected_variables$species)
set.seed(123) #setting the seed for reproducability 
#Running a random forest on my data set 
rf <- randomForest(species ~ ., data = penguins_selected_variables)
rf

```
Finding appropriate number of ntree 

```{r}
set.seed(123) #setting the seed for reproducability 
##force a large number of trees
rf <- randomForest(species ~ ., data = penguins_selected_variables, ntree=1000)
#extracting the OOB error rate from the rf 
head(rf$err.rate)
```
Plotting the OOB error rate against number of trees: 

```{r}
plot(rf$err.rate[,1], t='l')

```
The error rate is large for a single tree but plateaus as the number of trees increases. The error rate had already plateaud once 500 trees are reached therefore 500 is an appropriate number of trees to use, when mtree is the default. 

Checking the effect of different mtree values: 
Mtry is the number of variables used to decide the species for each tree that is used in the random forest model. The default value of mtry is the square root of the total number of variables in the dataset.  

```{r}
mtry_values<- c(1,2,3,4) #mtry_values is a vector containing the values of mtry which is the number of variables randomly sampled at each split 
ntree<- 1000 #we have set the number of trees to very high 
OOB_results<- c() #we have initialised an empty vector for the out of bound error results 
for(mt in mtry_values){ #this for loop will iterate over each value of mtry in mtry_values
  rf <- randomForest(species ~ ., data = penguins_selected_variables, ntree=ntree, mtry=mt) #for each value of mtry the for loop fits a random forest model to the dataframe penguins_selected_variables 
  ##OOB only
  newOOB<- rf$err.rate[,1] #and extracts the OOB error rates, and saves them in a vector called newOOB 
  ##save each OOB solution by row on a data.frame
  OOB_results<- rbind(OOB_results,newOOB) #these error rates are then appended as a new row to OOB_results using the rbind function 
}

plot(OOB_results[1,], t='l', col=1, ylim=c(0,0.1)) #plots the OOB error rate stored in OOB results using the error rates for the first value of mtry 
for(mt in 2:length(mtry_values)){ #this for loop adds lines to the plot for the error rates corresponding to the other values of mtry 
  lines(OOB_results[mt,], col=mt)
}
legend("topright",legend=mtry_values,col =1:length(mtry_values) ,lty=1)

```

The OOB error is noisy and varies between runs but its variation is small. As the number of trees increases, OOB error rate decreases, irrespective of of mtry. The out of bound error for a small value of ntree is large but OOB error rate plateaus once a value of 500 is reached for ntree irrespective of the value of mtry. The effect of mtry is visible however the OOB error for each mtry plateaus at approximately the same point so the default value of mtry, 2 (square root of 4) is appropriate for use. A value for mtry that is too high can increase risk of overfitting, especially if some of the variables are 'noisy' and not important in predicting penguin species. If a large number of variables are considered at each split the tree might capture noise or spurious correlations in the data leading to overfitting and poor generalization to unseen data. 



```{r}
#rerunning the rf model with the default parameters 
set.seed(123)
rf <- randomForest(species ~ ., data = penguins_selected_variables)
rf

```
Finding the most important variables: 


The random forest will quantify how important each variable was for the predicting what species each individual datapoint in the dataset belongs to.  We can extract and plot this information from the forest: 

```{r echo=TRUE, include=TRUE, fig.width=10, fig.height=4}
  
imp<- importance(rf) ##extracts a matrix with the importance information
print(imp)

varImpPlot(rf) ##plots the importance information

```

Bill length has the highest importance in predicting penguin species since it has the highest value for Mean decrease in Gini index. This is followed by flipper length, bill depth and body mass. 

Extracting the confusion matrix: 

```{r}
#extract confusion matrix
m<- rf$confusion
#remove the last column which is the error.class.  
m<- m[,-ncol(m)]
m


```
Calculating sensitivity and specificity for each penguin species 

Adelie: 

Sensitivity / True positive rate:

Rows denote the actual numbers of each species and columns denote predicted numbers of each species so the correctly predicted cases for Adelie is the cell m(1,1), and so on. 
The correctly predicted cases for each species is donated by the diagnonal across the confusion matrix, ie (m[1,1]), (m[2,2]), and (m[3,3]). 
```{r}
##calc sensitity (True Positive Rate)
#Rows denote the actual numbers of each species and columns denote predicted numbers of each species so correctly predicted species for Adelie is the cell m(1,1), and so on. 
TPR<- (m[1,1])/sum(m[1,]) #the numerator is the number of correctly predicted cases for Adelie. The denominator is the number of cases predicted to be Adelie. 
TPR

```
The true positive for adelie is 97%. 

Specificity / true negative 

The number of correctly predicted negatives divided by the actual number of negatives 


```{r}
TNR <- (m[2, 2] + m[3, 3]) / (sum(m[2, ]) + sum(m[3, ])) #the numerator sums the number of correct predictions for the species not to be Adelie. The denominator sums rows 2 and 3, the number of cases when the species was actually not adelie (ie gentoo or chinstrap.)
TNR
```
The true negative rate for Adelie is 97%. 

Chinstrap: 
Sensitivity / True positive rate 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[2,2])/sum(m[2,])
TPR

```
The true positive rate for chinstrap is 94%. 

Specificity / true negative rate 

```{r}
TNR <- (((m[1,1]) + (m[3,3]))) / (sum(m[1,]) + sum(m[3,]))
TNR

```
The true negative rate for chinstrap is 98% 

Gentoo: 
Calculating True positive / Sensitivity 

```{r}
##calc sensitity (True Positive Rate)
TPR<- (m[3,3])/sum(m[3,])
TPR

```
The true positive rate for Gentoo is 99% 


Calculating true negative / specificity 

```{r}
TNR <- (((m[1,1]) + (m[2,2])) / (sum(m[2,]) + sum(m[1,])))
TNR


```
The true negative rate for Gentoo is 96%. 

Calculating overall accuracy: 

```{r}
##calc accuracy (prop correctly classified)
ACC<- (m[1,1] + m[2,2] + m[3,3])/sum(m)
ACC

```
The overall accuracy is 97%. 


Are all the species classified equally well? 

No. The true positive and true negatives vary subtly between species. Gentoo has the highest true positive rate of 99% whilst Chinstrap has the lowest true positive rate of 94%. Gentoo has the lowest true negative rate of 96% whereas Chinstrap has the highest true negative rate of 98%. 


Bill length has the highest importance in predicting penguin species since it has the highest value for Mean decrease in Gini index. This is followed by flipper length, bill depth and body mass. 

```{r}
# Scatter plot of body mass against bill depth
ggplot(penguins_selected_variables, aes(x = bill_depth_mm, y = body_mass_g, color = species)) +
  geom_point() +
  labs(x = "Bill depth (mm)", y = "Body Mass (g)") +
  ggtitle("Scatter Plot of Bill depth vs. Body Mass")

```
This scatterplot demonstrates why bill depth and body mass are less important variables in predicting penguin species, because although the gentoo species clusters separrately, both the Adelie and the chinstrap points overlap and cluster together in morphological space because bill depth is not a good predictor for penguin species. 


```{r}
#Plotting Bill length against body mass 

# Scatter plot of bill length against body mass
ggplot(penguins_selected_variables, aes(x = bill_length_mm, y = body_mass_g, color = species)) + #plotting bill length on the x axis and body mass on the y axis where each individual is colored by species
  geom_point() +
  labs(x = "Bill Length (mm)", y = "Body Mass (g)") +
  ggtitle("Scatter Plot of Bill Length vs. Body Mass")

```

This scatter plot shows that bill length is an important feature in predicting species. When bill length is plotted against body mass, the adelie, chinstrrap and gentoo species cluster further apart from eachother, showing clearer boundaries in feature and morphological space, because the length of the Bill is an important feature differentiating between different species. However, there is still some overlap between the points of the Adelie and Gentoo species. This is because body mass is not a good predictor of penguin species. 


Plotting two important features against eachother; bill length against flipper length:

```{r}
#plotting bill length against flipper length 
ggplot(penguins_selected_variables, aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +
  geom_point() +
  labs(x = "Bill length (mm)", y = "Flipper length (mm)") +
  ggtitle("Scatter Bill length vs Flipper length")

```
Bill length and flipper length are the most important variables in predicting penguin species. When bill length and flipper length are plotted against eachother, the clearest boundaries are visible in morphological space between all three species, explaining why they are the most important variables in predicting penguin species. 




Question 4: 


CORRECTED version where it makes multiple augmented data sets

```{r}

# Define values of n where n is the number of noisy arrays to add to my dataset 
n_values <- c(0, 1, 5, 10, 20, 50, 75, 100) 

# Initialize a list to store the results for accuracy calculations for each data frame 
accuracy_results <- list()

#iterate over each value of n
for (n in n_values) {
  #initialize a vector to store accuracies for each value of n
  accuracies <- numeric()
  

  M <- 50 #Repeat the experiment M times where M is 50 
  for (i in 1:M) { #for every experiment out of M experiments 
    # Generate n noisy arrays where n is one of n_values, drawn randomly from the normal distribution
    noisy_arrays <- lapply(1:n, function(j) rnorm(N, 0, 1)) #where N is the length of the penguins_selected_variables_dataset
    
    #Augmenting our new noisy arrays to the penguins_selected_variables dataset creating multiple new datasets 
    augmented_dataset <- penguins_selected_variables  # Create a new dataset that is identical to my previous dataset 
    names(noisy_arrays) <- paste0("x", 1:length(noisy_arrays)) #naming the columns from 1 to the number of noisy arrays that I have
    augmented_dataset <- cbind(augmented_dataset, noisy_arrays) #binding the noisy arrays I have made to my new augmented dataset 
    
    # Make sure the variable we are predicting is a factor so the random forest works 
    augmented_dataset$species <- factor(augmented_dataset$species)
    
    # Train a random forest model on the augmented dataset
    rf <- randomForest(species ~ ., data = augmented_dataset, ntree = 2000)
    
    # Calculate accuracy and store it in accuracies vector
    m <- rf$confusion
    m <- m[,-ncol(m)]
    ACC <- (m[1,1] + m[2,2] + m[3,3])/sum(m)
    accuracies <- c(accuracies, ACC)
  }
  
  #This for loop will be completed 50 times for each value of n in n_values 
  
  # Calculate mean accuracy for this value of n
  mean_accuracy <- mean(accuracies)
  
  # Store mean accuracy in accuracy_results list
  accuracy_results[[as.character(n)]] <- mean_accuracy #because a list can only store character values 
}


# Convert accuracy_results to a data frame
accuracy_df <- data.frame(
  n = as.numeric(names(accuracy_results)),
  mean_accuracy = unlist(accuracy_results)
)


```

Plotting this data

```{r}


# Plot mean accuracy as a function of n
ggplot(accuracy_df, aes(x = n, y = mean_accuracy)) +
  geom_line() +
  geom_point() +
  labs(x = "n", y = "Mean Accuracy", title = "Mean Accuracy vs. Sample Size (n)")


```


As the number of noisy arrays added to the dataset increases, the accuracy of the random forest model decreases. This is because these noisy arrays being added to the dataset are random, meaningless and are not useful in predicting which species each individual belongs to. However, the random forest model incorporates these variables as they may exhibit random or spurious correlations with the target variable. The relevant information and true patterns becomes harder to discern amongst the growing noise in the dataset, so the model cannot distinguish between meaningful and meaningless variables leading to poorer predictive performance of the model. This is called overfitting. 





Question 5: 

Loading the data:

```{r}

covertype_data <- read_csv( "/Users/cissi.user.2/Documents/Computer coding/Random forest assignment/Data/covertype.csv")

head(covertype_data)
```
Creating two smaller data sets, data_train and data_test of sizes Ntrain = 1000 and Ntest = 1000 with data points selected at random from the full data set. Data_train and data_test will not share any data points.

Making the data_train and data_test 


```{r}
# Set the sizes for training and testing sets
Ntrain <- 10000
Ntest <- 1000


# Generate indices for all rows
total_rows <- nrow(covertype_data) #making a new object called total_rows where the number of rows is the number of rows in covertype data 
all_indices <- 1:total_rows

# Sample indices for data_train
train_indices <- sample(all_indices, Ntrain, replace = FALSE) #making an object called train indices by taking a sample from all indices where the size of the sample is the size of Ntrain 

# Making a new object called. remaining_indices, excluding train_indices from the pool of indices so that data_train and data_test do not share any data points. 
remaining_indices <- setdiff(all_indices, train_indices)

# Sample indices for data_test from the remaining indices
test_indices <- sample(remaining_indices, Ntest, replace = FALSE) #making a new vector called test_indices, by sampling from remaining_indices, where the number of rows is the size of Ntest. 

# Create data_train and data_test using the sampled indices
data_train <- covertype_data[train_indices, ] #creating an object called data_train using the train_indices vector to subset rows from covertype_data.  
data_test <- covertype_data[test_indices, ] #creating a new data frame called data_test using the test_indices vector to subset rows from the covertype_data by selecting rows corresponding to the indices contained in the test_indices vector 



data_train
data_test



```


```{r, echo = FALSE}

#The columns Wilderness_AreaX = 1,2,3,4 are one hot encoded where if a data point is such that Wilderness_Area1 = 1, the remaining Wilderness_AreaX values will be 0. The following code condenses this information into a single Wilderness_Area variable such that Wilderness_Area = X if Wilderness_AreaX = 1. The same has been done for the variable soil type. 

# Find the columns representing Wilderness_AreaX
wilderness_cols <- grep("Wilderness_Area", names(data_train)) #using the grep function to search for column names in data_train containing the string "wilderness_Area", and returning the indices of the column names that contain this string which will be stored in the wilderness_cols vector. 

# Get the column names without the Wilderness_Area prefix
wilderness_areas <- gsub("Wilderness_Area", "", names(data_train[wilderness_cols]))
#removes the "Wilderness_Area" prefix from each column name in the subsetted dataset, resulting in a vector of Wilderness_AreaX variable names. The modified names are stored in the wilderness_areas vector.

# Find the column index where Wilderness_AreaX = 1 for each row
wilderness_area_index <- apply(data_train[, wilderness_cols], 1, function(x) which(x == 1)) #making a new vector where each element corresponds to a row in data_train and each element contains the indices of the Wilderness_AreaX that have a value of 1 for each observation in the dataset.

# Extract the corresponding Wilderness_Area value
data_train$Wilderness_Area <- wilderness_areas[wilderness_area_index]

# Convert Wilderness_Area to a factor
data_train$Wilderness_Area <- factor(data_train$Wilderness_Area)

# Remove the original one-hot-encoded Wilderness_AreaX columns
data_train <- data_train[, -wilderness_cols]

#repeating this for the soil columns 

# Find the columns representing Soil_TypeX
soil_cols <- grep("Soil_Type", names(data_train))

# Get the column names without the Soil_Type prefix
soil_types <- gsub("Soil_Type", "", names(data_train[soil_cols]))

# Find the column index where Soil_TypeX = 1 for each row
soil_type_index <- apply(data_train[, soil_cols], 1, function(x) which(x == 1))

# Extract the corresponding Soil_Type value
data_train$Soil_Type <- soil_types[soil_type_index]

# Convert Soil_Type to a factor
data_train$Soil_Type <- factor(data_train$Soil_Type)

# Remove the original one-hot-encoded Soil_TypeX columns
data_train <- data_train[, -soil_cols]

head(data_train)


```

Running a random forest on data_train with the default parameters: 


```{r}
#Converting class to a factor so the random forest can work 
data_train$class <- factor(data_train$class)
set.seed(123)
#Running the random forest model on my data_train 
rf <- randomForest(class ~ ., data = data_train)
rf


```



Reporting variable importance: 

```{r}
imp<- importance(rf) ##extracts a matrix with the importance information
print(imp)

varImpPlot(rf) ##plots the importance information

```
The most important variable for predicting Forest cover type is Elevation. 
Extracting the confusion matrix: 


```{r}
str(rf$confusion)
#extract confusion matrix
m<- rf$confusion
#remove the last column which is the error.class.  ?
m<- m[,-ncol(m)]
m



```


Excracting information about accuracy, sensitivity and specificity of this random forest model 

```{r}
# Assuming m is your confusion matrix
m  

# Number of classes
num_classes <- nrow(m)

# Initialize vectors to store results
accuracy <- numeric(num_classes)
sensitivity <- numeric(num_classes)
specificity <- numeric(num_classes)

# Calculate metrics for each class
for (i in 1:num_classes) {
  # Accuracy
  accuracy[i] <- m[i, i] / sum(m[, i])
  
  # Sensitivity / True positive rate
  sensitivity[i] <- m[i, i] / sum(m[i, ])
  
  # True negative rate / Specificity
  true_negatives <- sum(diag(m)) - m[i, i]  # Subtract true positives from total true positives and true negatives
  total_negatives <- sum(m) - sum(m[i, ])   # Subtract true positives and false positives from total
  specificity[i] <- true_negatives / total_negatives
}

# Print or return the results
print(accuracy)
print(sensitivity)
print(specificity)

# Create a data frame with class and metrics
results_df <- data.frame(
  class = 1:num_classes,
  accuracy = accuracy,
  sensitivity = sensitivity,
  specificity = specificity
)

# Print or return the data frame
print(results_df)



```


```{r}

#for loop to extract sample sizes for each class 

# Get unique class labels
unique_classes <- unique(data_train$class)

# Create an empty vector to store sample sizes
sample_sizes <- numeric(length(unique_classes))

# Iterate over each unique class label
for (i in 1:length(unique_classes)) {
  # Count occurrences of the current class label
  class_count <- sum(data_train$class == unique_classes[i])
  # Store the sample size for the current class label
  sample_sizes[i] <- class_count
}

# Print the sample sizes for each class
for (i in 1:length(unique_classes)) {
  cat("Class", unique_classes[i], ": Sample Size =", sample_sizes[i], "\n")
}

```

Forest cover types 4, 5 and 6 are predicted correctly the least often as they have the lowest true positive rates. This means that the rate at which data points are conceptually defined as positive for these classes are predicted as positive is very low. Specifically forest cover type 5 has the lowest sensitivity of 0.24. 

Firstly, class 5 has a small sample size of 158 in data_train (the dataset used to train the random forest model). If this class is underepresented in data_train this could lead to higher rates of misclassification. 
There are also a large number of variables that are used to predict forest type. Although the most important variable is Elevation, there are a large number of variables whose mean decrease in Gini Index is smaller such as Hillshade_9am, Hillshade_3pm, Slope, Aspect, and more. These variables are not useful in predicting which class which individual tree belongs to. However the random forest model incorporates these variables. This causes the relevant information and true patterns to become harder to discern so the random forest model cannot distinguish between meaningful and meaningless variables leading to poorer predictive performance of the model. This is called over fitting.





```{r, echo = FALSE}
#data wrangling on data_test so that the structure of data_test and data_train are the same
#removing the one hot coded variables from data_test so that the structure of the datasets data_train and data_test are the same 
# Find the columns representing Wilderness_AreaX
wilderness_cols <- grep("Wilderness_Area", names(data_test))

# Get the column names without the Wilderness_Area prefix
wilderness_areas <- gsub("Wilderness_Area", "", names(data_test[wilderness_cols]))

# Find the column index where Wilderness_AreaX = 1 for each row
wilderness_area_index <- apply(data_test[, wilderness_cols], 1, function(x) which(x == 1))

# Extract the corresponding Wilderness_Area value
data_test$Wilderness_Area <- wilderness_areas[wilderness_area_index]

# Convert Wilderness_Area to a factor
data_test$Wilderness_Area <- factor(data_test$Wilderness_Area)

# Remove the original one-hot-encoded Wilderness_AreaX columns
data_test <- data_test[, -wilderness_cols]

#repeating this for the soil columns 

# Find the columns representing Soil_TypeX
soil_cols <- grep("Soil_Type", names(data_test))

# Get the column names without the Soil_Type prefix
soil_types <- gsub("Soil_Type", "", names(data_test[soil_cols]))

# Find the column index where Soil_TypeX = 1 for each row
soil_type_index <- apply(data_test[, soil_cols], 1, function(x) which(x == 1))

# Extract the corresponding Soil_Type value
data_test$Soil_Type <- soil_types[soil_type_index]

# Convert Soil_Type to a factor
data_test$Soil_Type <- factor(data_test$Soil_Type)

# Remove the original one-hot-encoded Soil_TypeX columns
data_test <- data_test[, -soil_cols]

head(data_test)


```

```{r, echo = FALSE}
#final data wrangling on data_test so the structure of data_train and data_test are the same 
# Check column names
identical_colnames <- identical(colnames(data_train), colnames(data_test))
if (!identical_colnames) {
  stop("Column names in data_train and data_test do not match.")
}

# Check and adjust factor levels if needed
for (var in names(data_test)) {
  if (is.factor(data_test[[var]])) {
    levels_train <- levels(data_train[[var]])
    levels_test <- levels(data_test[[var]])
    if (!identical(levels_train, levels_test)) {
      warning(paste("Factor levels for", var, "in data_test differ from data_train. Adjusting..."))
      data_test[[var]] <- factor(data_test[[var]], levels = levels_train)
    }
  }
}

```


Using the random forest to predict Forest cover type in my other dataset, data_test:

```{r}
#Converting class to a factor so the random forest can work 
data_test$class <- factor(data_test$class)

#making predictions on the test data using the random forest I just made 
predictions <- predict(rf, newdata = data_test, type = "class")
#printing the predictions 
print(predictions)

```

The following code will extract information about how successfully the random forest model performs on the test data, data_test. 

```{r}

predictions_info <- confusionMatrix(table(predictions, data_test$class)) # The prediction to compute the confusion matrix and see the accuracy score
predictions_info

# Extract the confusion matrix table
m <- predictions_info$table

# Print the confusion matrix table
print(m)


#calculating the number of correctly predicted points from this 

correctly_predicted_points <- (m[1,1]) + (m[2,2]) + (m[3,3]) + (m[4,4]) + (m[5,5]) + (m[6,6]) + (m[7,7])
correctly_predicted_points 




```

There are 823 correctly predicted points.





